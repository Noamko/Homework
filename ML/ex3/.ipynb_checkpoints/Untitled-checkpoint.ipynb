{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a6976f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "21c302d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training data...\n",
      "done loading training data.\n"
     ]
    }
   ],
   "source": [
    "print(\"loading training data...\")\n",
    "train_x = np.loadtxt('train_x')\n",
    "train_x = np.true_divide(train_x, 255)\n",
    "train_y = np.loadtxt('train_y')\n",
    "test_x = np.loadtxt(\"test_x\")\n",
    "print(\"done loading training data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f3f1ccde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffeling & cutting.\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "print(\"shuffeling & cutting.\")\n",
    "training_set = list(zip(train_x,train_y))\n",
    "np.random.shuffle(training_set)\n",
    "training_set = training_set[:5000]\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3925f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noam Koren\n",
    "# 308192871\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"TODO:\n",
    "add more hidden layers\n",
    "try to optimize\n",
    "refactor to be more generic\n",
    "\"\"\"\n",
    "\n",
    "class NN:\n",
    "    #initialize weights and biases\n",
    "    def __init__(self,inputLayerSize, hiddenLayerSize, outputLayerSize, hiddenLayerCount = 1):\n",
    "        self.epochs = 20\n",
    "        self.learning_rate = 0.02\n",
    "        self.weights = [np.random.uniform(-0.09, 0.09, [hiddenLayerSize, inputLayerSize])]\n",
    "        self.biases = [np.random.uniform(-0.09, 0.09, [hiddenLayerSize, 1])]\n",
    "        #initialize weights and biases with random values\n",
    "\n",
    "        for i in range(1, hiddenLayerCount):\n",
    "            self.weights.append(np.random.uniform(-0.09, 0.09, [hiddenLayerSize, hiddenLayerSize]))\n",
    "            self.biases.append(np.random.uniform(-0.09, 0.09, [hiddenLayerSize, 1]))\n",
    "\n",
    "        self.weights.append(np.random.uniform(-0.09, 0.09, [outputLayerSize, hiddenLayerSize]))\n",
    "        self.biases.append(np.random.uniform(-0.09, 0.09, [outputLayerSize, 1]))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x.reshape(len(x), 1)\n",
    "        zs = [x]\n",
    "        hs = [x]\n",
    "        z1 = np.dot(self.weights[0], x) + self.biases[0]\n",
    "        zs.append(z1)\n",
    "        hs.append(self.sigmoid(z1))\n",
    "\n",
    "        for w, b in zip(self.weights[1:-1], self.biases[1:-1]):\n",
    "            z = np.dot(w, hs[-1]) + b\n",
    "            zs.append(z)\n",
    "            hs.append(self.sigmoid(z))\n",
    "        z2 = np.dot(self.weights[-1], hs[-1]) + self.biases[-1]\n",
    "        zs.append(z2)\n",
    "        a = self.softmax(z2)\n",
    "        hs.append(a)\n",
    "        return (zs, hs)\n",
    "\n",
    "    def backprob(self, forward_result, y):\n",
    "        dws = []\n",
    "        dbs = []\n",
    "        zs, hs = forward_result\n",
    "        dz2 = (self.softmax(zs[-1])) # TODO: check what is the real derivative for now this works fine\n",
    "        dz2[int(y)] -= 1\n",
    "        dws.append(np.matmul(dz2, hs[-2].T))\n",
    "        dbs.append(dz2)\n",
    "        dz = dz2\n",
    "        for i in range(2, len(self.weights)+1):\n",
    "            z = zs[-i]\n",
    "            dz = np.matmul(self.weights[-i+1].T, dz) * self.sigmoid_derivative(z)\n",
    "            dws.append(np.matmul(dz, hs[-i-1].T))\n",
    "            dbs.append(dz)\n",
    "        # dz1 = np.matmul(self.weights[-1].T, dz2) * self.sigmoid_derivative(zs[1])\n",
    "        # dws.append(np.matmul(dz1, zs[0].T))\n",
    "        # dbs.append(dz1)\n",
    "        dws.reverse()\n",
    "        dbs.reverse()\n",
    "        return (dws, dbs)\n",
    "\n",
    "    def softmax(self,x):\n",
    "        x = x - np.max(x)\n",
    "        return np.exp(x) / sum(np.exp(x))\n",
    "    def sigmoid(self, v):\n",
    "        v = np.clip(v,-500, 500)\n",
    "        return np.array([1 / (1 + np.exp(-x)) for x in v])\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        a = self.sigmoid(x)\n",
    "        return a * (1 - a)\n",
    "\n",
    "    def train(self,x,y):\n",
    "        training_set = list(zip(x,y))\n",
    "        for i in range(self.epochs):\n",
    "            np.random.shuffle(training_set)\n",
    "            print(\"training: epoch: \" + str(i) + \"/20\")\n",
    "            for example, label in training_set:\n",
    "                dws, dbs = self.backprob(self.forward(example),label)\n",
    "\n",
    "                for i in range (len(self.weights)):\n",
    "                    self.weights[i] -= self.learning_rate * dws[i]\n",
    "                    self.biases[i] -= self.learning_rate * dbs[i]\n",
    "\n",
    "    def predict(self,x):\n",
    "        prediction_vector = nn.forward(x)[1][-1]\n",
    "        return prediction_vector.argmax()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "94c02970",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NN(784, 150, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "515cb4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x,y = zip(*training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8d4171f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: epoch: 0/20\n",
      "training: epoch: 1/20\n",
      "training: epoch: 2/20\n",
      "training: epoch: 3/20\n",
      "training: epoch: 4/20\n",
      "training: epoch: 5/20\n",
      "training: epoch: 6/20\n",
      "training: epoch: 7/20\n",
      "training: epoch: 8/20\n",
      "training: epoch: 9/20\n",
      "training: epoch: 10/20\n",
      "training: epoch: 11/20\n",
      "training: epoch: 12/20\n",
      "training: epoch: 13/20\n",
      "training: epoch: 14/20\n",
      "training: epoch: 15/20\n",
      "training: epoch: 16/20\n",
      "training: epoch: 17/20\n",
      "training: epoch: 18/20\n",
      "training: epoch: 19/20\n"
     ]
    }
   ],
   "source": [
    "nn.train(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c09b2a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.predict(test_x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076116fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
